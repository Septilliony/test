{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m  \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_seq2seq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer, load_chinese_base_vocab\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_seq2seq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_bert\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\dengyi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\bert_seq2seq\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_chinese_base_vocab, Tokenizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_bert, load_gpt\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5_ch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5PegasusTokenizer, T5Model\n",
      "File \u001b[1;32mc:\\users\\dengyi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\bert_seq2seq\\tokenizer.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjieba\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_chinese_base_vocab\u001b[39m(vocab_path, simplfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, startswith\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    加载官方中文bert模型字典\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    simplified: 是否简化词典\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bert_seq2seq import Tokenizer, load_chinese_base_vocab\n",
    "from bert_seq2seq import load_bert\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "vocab_path = \"./state_dict/roberta_wwm_vocab.txt\"  # roberta模型字典的位置 Whole Word Masking\n",
    "word2idx = load_chinese_base_vocab(vocab_path)  # 词典\n",
    "model_name = \"roberta\"  # 选择模型名字\n",
    "model_path = \"./state_dict/roberta_wwm_pytorch_model.bin\"  # 模型位置 Whole Word Masking\n",
    "recent_model_path = \"./state_dict/bert_auto_title_model.bin\"   # 用于把已经训练好的模型继续训练\n",
    "\n",
    "model_save_path = \"./state_dict/auto_save.bin\"\n",
    "train_data_path = './dataset/train_data.csv'\n",
    "val_data_path = './dataset/val_data.csv'\n",
    "test_data_path = './dataset/test_data.csv'\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "lr = 1e-5\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    \"\"\"\n",
    "    针对特定数据集，定义一个相关的取数据的方式\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path):\n",
    "        super(BertDataset, self).__init__()\n",
    "        data = pd.read_csv(data_path)\n",
    "        ids = data.loc[:, 'id']\n",
    "        findings = data.loc[:, 'clear_finding']\n",
    "        impressions = data.loc[:, 'empression']\n",
    "        # 读取数据并转列表\n",
    "\n",
    "        self.id_list = np.array(ids).tolist()\n",
    "        self.finding_list = np.array(findings).tolist()\n",
    "        self.impression_list = np.array(impressions).tolist()\n",
    "        self.idx2word = {k: v for v, k in word2idx.items()}\n",
    "        self.tokenizer = Tokenizer(word2idx)\n",
    "\n",
    "    def __getitem__(self, i):        \n",
    "        if (len(self.finding_list[i]) > 1):\n",
    "            \n",
    "            finding = self.finding_list[i]\n",
    "            impression = self.impression_list[i]\n",
    "            token_ids, token_type_ids = self.tokenizer.encode(\n",
    "                finding, impression, max_length=maxlen\n",
    "            )  \n",
    "            output = {\n",
    "                \"token_ids\": token_ids,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "            }\n",
    "            return output\n",
    "        # 若 if 没有被执行 说明该数据不合法，直接返回下一个数据\n",
    "        return self.__getitem__(i + 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.finding_list)\n",
    "\n",
    "    def get_line(self, i):\n",
    "        output = {\n",
    "            'id': self.id_list[i],\n",
    "            'finding': self.finding_list[i],\n",
    "            'empression': self.impression_list[i]\n",
    "        }\n",
    "        return output\n",
    "        \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    动态 padding 对一个 batch 进行处理。Transform 是对单个数据进行处理。\n",
    "    一个批次是 16 组数据\n",
    "    \"\"\"\n",
    "\n",
    "    def padding(indice, max_length, pad_idx=0):\n",
    "        \"\"\"\n",
    "        pad 函数\n",
    "        \"\"\"\n",
    "        pad_indice = [item + [pad_idx] * max(0, max_length - len(item)) for item in indice]\n",
    "        return torch.tensor(pad_indice)\n",
    "\n",
    "    token_ids = [data[\"token_ids\"] for data in batch]  # 取出 token_ids 列表\n",
    "    max_length = max([len(t) for t in token_ids])\n",
    "    token_type_ids = [data[\"token_type_ids\"] for data in batch]  # 取出 type_ids 列表\n",
    "\n",
    "    token_ids_padded = padding(token_ids, max_length)  # 对齐 token_ids 和 type_ids 长度\n",
    "    token_type_ids_padded = padding(token_type_ids, max_length)\n",
    "    # 删掉 token_ids 的第一列（[cls]：101）\n",
    "    target_ids_padded = token_ids_padded[:, 1:].contiguous()  \n",
    "\n",
    "    return token_ids_padded, token_type_ids_padded, target_ids_padded\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model_path):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device: \" + str(self.device))\n",
    "        # 定义模型 model_class 默认值为 seq2seq 故本行代码可省略 model_class 部分\n",
    "        self.bert_model = load_bert(word2idx, model_name=model_name)\n",
    "\n",
    "        self.bert_model.load_pretrain_params(model_path)\n",
    "        self.bert_model.set_device(self.device)\n",
    "\n",
    "        self.optim_parameters = list(self.bert_model.parameters())\n",
    "        self.optimizer = torch.optim.Adam(self.optim_parameters, lr=lr, weight_decay=1e-3)\n",
    "\n",
    "        train_dataset = BertDataset(train_data_path)\n",
    "        val_dataset = BertDataset(val_data_path)\n",
    "        # test_dataset = BertDataset(test_data_path)\n",
    "\n",
    "        self.train_loader =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        self.val_loader =  DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        # self.test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        \n",
    "        \n",
    "        self.val_loss_list = []\n",
    "        self.train_loss_list = []\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        train_loss = self.train_loss_list\n",
    "        train_x = list(range(len(train_loss)))\n",
    "\n",
    "        plt.title('train_loss_per_epoch')\n",
    "        plt.plot(train_x, train_loss)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.savefig('train_loss.jpg', dpi=300) #指定分辨率保存\n",
    "\n",
    "        validate_loss = self.val_loss_list\n",
    "        val_x = list(range(len(validate_loss)))\n",
    "        plt.title('val_loss_per_epoch')\n",
    "        plt.plot(val_x, validate_loss)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.savefig('eval_loss.jpg', dpi=300) #指定分辨率保存\n",
    "\n",
    "\n",
    "    def train(self, epoch):\n",
    "        # 一个 epoch 的训练\n",
    "        self.bert_model.train()\n",
    "        self.train_iter(epoch, dataloader=self.train_loader, train=True)\n",
    "        self.bert_model.eval()\n",
    "        self.val_iter(epoch, dataloader=self.val_loader, train=False)\n",
    "        # self.val_iter(epoch, dataloader=self.val_loader_2, train=False)\n",
    "\n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        保存模型\n",
    "        \"\"\"\n",
    "        self.bert_model.save_all_params(save_path)\n",
    "        print(\"{} saved. \".format(save_path))\n",
    "\n",
    "\n",
    "    def val_iter(self, epoch, dataloader, train=False):\n",
    "        total_loss = 0\n",
    "        print(f'epoch: {epoch}')\n",
    "        start_time = time.time()  # 得到当前时间\n",
    "        for token_ids, token_type_ids, target_ids in tqdm(dataloader, position=0, leave=True):\n",
    "            # 因为传入了 target 标签，因此会计算 loss 并且返回\n",
    "            predictions, loss = self.bert_model(token_ids,\n",
    "                                                token_type_ids,\n",
    "                                                labels=target_ids,                                               \n",
    "                                                )\n",
    "            total_loss += loss.item()\n",
    "   \n",
    "        end_time = time.time()\n",
    "        spend_time = end_time - start_time\n",
    "        self.val_loss_list.append(total_loss)\n",
    "        with open('./loss/val_loss.txt', 'a') as f:\n",
    "            f.write(f'epoch: {epoch}. total_loss: {total_loss:.3f}\\n')\n",
    "        print(\"validate epoch is \" + str(epoch)+\". loss is \" + str(total_loss) + \". spend time is \"+ str(spend_time))\n",
    "       \n",
    "\n",
    "    def train_iter(self, epoch, dataloader, train=True):  # 迭代\n",
    "        total_loss = 0\n",
    "        print(f'epoch: {epoch}')\n",
    "        start_time = time.time()  # 得到当前时间\n",
    "        for token_ids, token_type_ids, target_ids in tqdm(dataloader, position=0, leave=True):\n",
    "            # 因为传入了 target 标签，因此会计算 loss 并且返回\n",
    "            predictions, loss = self.bert_model(token_ids,\n",
    "                                                token_type_ids,\n",
    "                                                labels=target_ids,                                               \n",
    "                                                )\n",
    "            # 反向传播 清空之前的梯度\n",
    "            self.optimizer.zero_grad()\n",
    "            # 反向传播, 获取新的梯度\n",
    "            loss.backward()\n",
    "            # 用获取的梯度更新模型参数\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # 为计算当前epoch的平均loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        spend_time = end_time - start_time\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.save(f\"./state_dict/auto_save_epoch_{epoch}.bin\")\n",
    "\n",
    "        self.train_loss_list.append(total_loss)\n",
    "        with open('./loss/train_loss.txt', 'a') as f:\n",
    "            f.write(f'epoch: {epoch}. total_loss: {total_loss:.3f}\\n')\n",
    "        print(\"train epoch is \" + str(epoch)+\". loss is \" + str(total_loss) + \". spend time is \"+ str(spend_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict/auto_save_epoch_0.bin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m(model_path\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[0;32m      4\u001b[0m data_set \u001b[38;5;241m=\u001b[39m BertDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/test_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mlen\u001b[39m(data_set)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = 'state_dict/auto_save_epoch_0.bin'\n",
    "trainer = Trainer(model_path=model_path)\n",
    "\n",
    "data_set = BertDataset('dataset/test_data.csv')\n",
    "\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total make csv time: 5151.6\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "with open(f'{model_path}.csv', mode='a',encoding='utf-8', newline='') as f:\n",
    "    csv_writer = csv.DictWriter(f,fieldnames=['id', 'finding', 'empression', 'generate_empression'])#列名\n",
    "    csv_writer.writeheader()  #列名写入csv\n",
    "    time1 = time.time()\n",
    "    for i in range(len(data_set)):\n",
    "        dic = (data_set.get_line(i))\n",
    "        gene = trainer.bert_model.generate(dic['finding'])\n",
    "        dic['generate_empression'] = gene\n",
    "        csv_writer.writerow(dic)   #数据写入csv文件\n",
    "    print(f'Total make csv time: {(time.time() - time1):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "with open(f'{model_path}.csv', mode='a',encoding='utf-8', newline='') as f:\n",
    "    data_list = []\n",
    "\n",
    "    for i in range(500):\n",
    "        dic = (data_set.get_line(i))\n",
    "        gene = trainer.bert_model.generate(dic['finding'])\n",
    "        dic['generate_empression'] = gene\n",
    "        data_list.append(dic)\n",
    "        \n",
    "    df = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>finding</th>\n",
       "      <th>empression</th>\n",
       "      <th>generate_empression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>两肺肺纹理增多、增粗。</td>\n",
       "      <td>支气管炎改变，请结合临床。\\r\\n</td>\n",
       "      <td>两肺未见明显实质性病变。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>右肺见钙化影；心影增大呈主动脉型。</td>\n",
       "      <td>提示心脏增大。\\n</td>\n",
       "      <td>两肺未见明显实质性病变；心影增大，请结合临床。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>两肺肺纹理增粗、紊乱、模糊。</td>\n",
       "      <td>胸部所示符合支气管炎改变。</td>\n",
       "      <td>胸部所示符合支气管炎改变。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>心影饱满。</td>\n",
       "      <td>两肺未见明显实质性病变；\\r\\n心影饱满。</td>\n",
       "      <td>两肺未见明显实质性病变；心影饱满，请结合临床。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>两肺肺纹理增多、增粗。</td>\n",
       "      <td>两肺心膈未见明显异常。</td>\n",
       "      <td>两肺未见明显实质性病变。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1057</td>\n",
       "      <td>两肺肺纹理增多、模糊；肺野透亮度增高，左肺下野、右肺下野见网格状密度影；两侧肺门增浓；两侧膈...</td>\n",
       "      <td>慢性支气管疾患伴感染、肺气肿。</td>\n",
       "      <td>提示慢性支气管疾患、肺气肿，请结合临床。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1058</td>\n",
       "      <td>两肺肺纹理增多、增粗。</td>\n",
       "      <td>两肺未见明显异常，请结合临床，随诊。</td>\n",
       "      <td>两肺纹理增多、增粗。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1059</td>\n",
       "      <td>两肺肺纹理增多、紊乱；心影增大呈主动脉型，心影饱满；主动脉迂曲，主动脉增宽，主动脉球部钙化；...</td>\n",
       "      <td>1、慢支征象。\\r\\n2、心影呈动脉硬化样改变。\\r\\n3、右侧5/6肋骨陈旧性骨折。</td>\n",
       "      <td>两肺未见明显实质性病变；主动脉粥样硬化；请结合临床。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1062</td>\n",
       "      <td>心影增大。</td>\n",
       "      <td>1、双侧肺纹理增多，随访；\\r\\n2、心影横径增大，请结合临床。</td>\n",
       "      <td>两肺未见明显实质性病变；心影增大，请结合临床。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1065</td>\n",
       "      <td>两肺肺纹理增多、模糊；左侧膈面上提。</td>\n",
       "      <td>支气管炎改变，请结合临床。</td>\n",
       "      <td>两肺纹理增多，左侧膈面抬高，请结合临床。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            finding  \\\n",
       "0       1                                        两肺肺纹理增多、增粗。   \n",
       "1       2                                  右肺见钙化影；心影增大呈主动脉型。   \n",
       "2       3                                     两肺肺纹理增粗、紊乱、模糊。   \n",
       "3       4                                              心影饱满。   \n",
       "4       5                                        两肺肺纹理增多、增粗。   \n",
       "..    ...                                                ...   \n",
       "495  1057  两肺肺纹理增多、模糊；肺野透亮度增高，左肺下野、右肺下野见网格状密度影；两侧肺门增浓；两侧膈...   \n",
       "496  1058                                        两肺肺纹理增多、增粗。   \n",
       "497  1059  两肺肺纹理增多、紊乱；心影增大呈主动脉型，心影饱满；主动脉迂曲，主动脉增宽，主动脉球部钙化；...   \n",
       "498  1062                                              心影增大。   \n",
       "499  1065                                 两肺肺纹理增多、模糊；左侧膈面上提。   \n",
       "\n",
       "                                      empression         generate_empression  \n",
       "0                              支气管炎改变，请结合临床。\\r\\n                两肺未见明显实质性病变。  \n",
       "1                                    　　提示心脏增大。\\n     两肺未见明显实质性病变；心影增大，请结合临床。  \n",
       "2                                  胸部所示符合支气管炎改变。               胸部所示符合支气管炎改变。  \n",
       "3                          两肺未见明显实质性病变；\\r\\n心影饱满。     两肺未见明显实质性病变；心影饱满，请结合临床。  \n",
       "4                                    两肺心膈未见明显异常。                两肺未见明显实质性病变。  \n",
       "..                                           ...                         ...  \n",
       "495                              慢性支气管疾患伴感染、肺气肿。        提示慢性支气管疾患、肺气肿，请结合临床。  \n",
       "496                           两肺未见明显异常，请结合临床，随诊。                  两肺纹理增多、增粗。  \n",
       "497  1、慢支征象。\\r\\n2、心影呈动脉硬化样改变。\\r\\n3、右侧5/6肋骨陈旧性骨折。  两肺未见明显实质性病变；主动脉粥样硬化；请结合临床。  \n",
       "498             1、双侧肺纹理增多，随访；\\r\\n2、心影横径增大，请结合临床。     两肺未见明显实质性病变；心影增大，请结合临床。  \n",
       "499                                支气管炎改变，请结合临床。        两肺纹理增多，左侧膈面抬高，请结合临床。  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a14faf9cc757baa5048231696aa314315d0edd2199f6156de0d976589e20c67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
